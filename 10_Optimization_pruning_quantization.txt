#!/usr/bin/env python3
"""
Applying Model Optimization Techniques (Pruning & Quantization) to DistilBERT

Usage:
    python optimize_distilbert.py
"""

import os
import time
import copy
import tempfile
import psutil
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from transformers import DistilBertTokenizerFast, DistilBertForMaskedLM

# ---------- Config ----------
MODEL_NAME = "distilbert-base-uncased"
SAMPLE_TEXT = "The quick brown fox jumps over the lazy dog."
WARMUP = 5
ITER = 40
PRUNE_AMOUNT = 0.3  # 30% weights removed (change to 0.5 etc.)
DEVICE = torch.device("cpu")  # runs on CPU; change to cuda if you want GPU measurements
# ----------------------------

def get_process_memory_mb():
    p = psutil.Process(os.getpid())
    return p.memory_info().rss / (1024 * 1024)

def measure_inference_latency(model, tokenizer, text=SAMPLE_TEXT, device=DEVICE, warmup=WARMUP, iterations=ITER):
    model.to(device)
    model.eval()
    inputs = tokenizer(text, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # warmup
    with torch.no_grad():
        for _ in range(warmup):
            _ = model(**inputs)

    # measure
    times = []
    with torch.no_grad():
        for _ in range(iterations):
            t0 = time.time()
            _ = model(**inputs)
            t1 = time.time()
            times.append(t1 - t0)
    avg = sum(times) / len(times)
    p50 = sorted(times)[len(times)//2]
    return avg, p50

def model_size_in_mb(model, save_state_dict=True):
    # saves either state_dict or full model temporarily and returns file size in MB
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp_path = tmp.name
    try:
        if save_state_dict:
            torch.save(model.state_dict(), tmp_path)
        else:
            torch.save(model, tmp_path)
        size_mb = os.path.getsize(tmp_path) / (1024 * 1024)
    finally:
        os.remove(tmp_path)
    return size_mb

def apply_unstructured_global_pruning(model, amount=PRUNE_AMOUNT):
    # prune all nn.Linear weight tensors globally by magnitude
    parameters_to_prune = []
    for module_name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            parameters_to_prune.append((module, "weight"))

    # apply global unstructured pruning
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=amount,
    )
    # remove reparameterization to make pruning permanent (prune.remove)
    for module, _ in parameters_to_prune:
        try:
            prune.remove(module, "weight")
        except Exception:
            pass
    return model

def apply_dynamic_quantization(model):
    # quantize Linear layers dynamically to int8
    q_model = torch.quantization.quantize_dynamic(
        model, {nn.Linear}, dtype=torch.qint8
    )
    return q_model

def main():
    print("Loading tokenizer and model:", MODEL_NAME)
    tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)
    model = DistilBertForMaskedLM.from_pretrained(MODEL_NAME)
    model.to(DEVICE)

    print("\nBaseline measurements")
    mem_before = get_process_memory_mb()
    avg_lat, p50 = measure_inference_latency(model, tokenizer)
    mem_after = get_process_memory_mb()
    size_mb = model_size_in_mb(model, save_state_dict=True)
    print(f"RAM (process) before model load (MB): {mem_before:.1f}")
    print(f"RAM (process) after baseline run (MB): {mem_after:.1f}")
    print(f"Baseline avg latency (s): {avg_lat:.4f}, p50 (s): {p50:.4f}")
    print(f"Baseline model state_dict size (MB): {size_mb:.2f}")

    # 1) Pruning
    print("\nApplying global unstructured magnitude pruning (amount = {:.2f})".format(PRUNE_AMOUNT))
    pruned_model = copy.deepcopy(model)
    pruned_model = apply_unstructured_global_pruning(pruned_model, amount=PRUNE_AMOUNT)
    pruned_model.to(DEVICE)
    avg_lat_pruned, p50_pruned = measure_inference_latency(pruned_model, tokenizer)
    size_mb_pruned = model_size_in_mb(pruned_model, save_state_dict=True)
    mem_after_pruned = get_process_memory_mb()
    print(f"Pruned avg latency (s): {avg_lat_pruned:.4f}, p50 (s): {p50_pruned:.4f}")
    print(f"Pruned model state_dict size (MB): {size_mb_pruned:.2f}")
    print(f"RAM (process) after pruning run (MB): {mem_after_pruned:.1f}")

    # 2) Quantization (dynamic) on baseline
    print("\nApplying dynamic quantization to baseline model")
    q_model = apply_dynamic_quantization(copy.deepcopy(model))
    q_model.to(DEVICE)
    avg_lat_q, p50_q = measure_inference_latency(q_model, tokenizer)
    size_mb_q = model_size_in_mb(q_model, save_state_dict=True)
    mem_after_quant = get_process_memory_mb()
    print(f"Quantized avg latency (s): {avg_lat_q:.4f}, p50 (s): {p50_q:.4f}")
    print(f"Quantized model state_dict size (MB): {size_mb_q:.2f}")
    print(f"RAM (process) after quantized run (MB): {mem_after_quant:.1f}")

    # 3) Prune -> Quantize (apply quantization after pruning)
    print("\nApplying quantization after pruning (Prune -> Quantize)")
    pruned_then_q = apply_dynamic_quantization(copy.deepcopy(pruned_model))
    pruned_then_q.to(DEVICE)
    avg_lat_pq, p50_pq = measure_inference_latency(pruned_then_q, tokenizer)
    size_mb_pq = model_size_in_mb(pruned_then_q, save_state_dict=True)
    mem_after_pq = get_process_memory_mb()
    print(f"Pruned->Quantized avg latency (s): {avg_lat_pq:.4f}, p50 (s): {p50_pq:.4f}")
    print(f"Pruned->Quantized model state_dict size (MB): {size_mb_pq:.2f}")
    print(f"RAM (process) after pruned->quant run (MB): {mem_after_pq:.1f}")

    # Summary
    print("\nSummary (lower latency & lower size are better):")
    print(f"Baseline: latency {avg_lat:.4f}s, size {size_mb:.2f} MB")
    print(f"Pruned ({PRUNE_AMOUNT*100:.0f}%): latency {avg_lat_pruned:.4f}s, size {size_mb_pruned:.2f} MB")
    print(f"Quantized (dynamic): latency {avg_lat_q:.4f}s, size {size_mb_q:.2f} MB")
    print(f"Pruned -> Quantized: latency {avg_lat_pq:.4f}s, size {size_mb_pq:.2f} MB")

    print("\nNotes:")
    print(" - Dynamic quantization often gives good model size reduction and can improve CPU inference speed.")
    print(" - Unstructured pruning reduces parameter count but may not always translate to CPU speedups unless the sparse representation and kernels are optimized (framework/hardware dependent).")
    print(" - For stronger production gains consider structured pruning, knowledge distillation, or exporting to optimized runtimes (ONNX + int8) and/or using GPU with tensor-cores and fused kernels.")

if __name__ == "__main__":
    main()








#
Title: 
Applying Model Optimization Techniques (Pruning & Quantization) to a Pre-trained  Generative Model 
Objective: 
To apply model optimization techniques such as pruning and quantization to a small pre trained generative model (e.g., DistilBERT) and analyze their impact on inference speed and  memory usage on a local system. 
Problem Statement: 
Large generative AI models are computationally expensive and resource-intensive, limiting  their deployment on edge devices or low-resource environments. To address this, model  optimization techniques like pruning and quantization are used to reduce model size and  improve efficiency without significantly compromising performance. This experiment  involves applying such techniques to a pre-trained generative model and evaluating their  effects. 
Outcome: 
By the end of this experiment, students will be able to: 
∙ Understand the principles of model pruning and quantization. 
∙ Apply these techniques to a pre-trained generative model using Python libraries. 
∙ Measure and compare inference time, model size, and memory usage before and  after optimization. 
Theory: 
1. What is Model Optimization? 
Model optimization refers to techniques that reduce the computational and memory  requirements of neural networks while preserving acceptable levels of accuracy. These  methods make it feasible to run models in real-time and on edge devices. 
2. Key Techniques: 
A. Pruning: 
Pruning involves removing unnecessary weights or neurons from a neural network,  especially those with minimal contribution to the output. There are two main types: 
∙ Unstructured pruning: Removes individual weights.
∙ Structured pruning: Removes entire neurons, filters, or layers. 
Pruning can lead to: 
∙ Reduced model size 
∙ Faster inference 
∙ Slight loss in accuracy (if not fine-tuned post-pruning) 
Example library: torch.nn.utils.prune (PyTorch) 
B. Quantization: 
Quantization reduces the precision of model weights and activations, typically from 32-bit  floating point to 8-bit integers. This makes the model lighter and faster for inference. 
Types: 
∙ Post-Training Quantization: Applied after training. 
∙ Quantization-Aware Training (QAT): Includes quantization in the training loop. Benefits: 
∙ Reduces memory footprint 
∙ Speeds up inference 
∙ Maintains high accuracy when done properly 
Example libraries: torch.quantization, ONNX Runtime, TensorFlow Lite 3. Workflow Overview 
1. Load a pre-trained model (e.g., DistilBERT or T5). 
2. Apply pruning to remove low-importance weights. 
3. Apply quantization to reduce weight precision. 
4. Run inference before and after optimization. 
5. Measure and compare: 
o Inference time (e.g., using time or timeit) 
o Model size on disk 
4. Why Optimization is Important in Generative AI? 
∙ Deployability: Makes it easier to deploy on mobile, web, or IoT. 
∙ Responsiveness: Faster responses improve user experience. 
∙ Energy efficiency: Reduced computational load means less power usage. ∙ Scalability: Serve more users simultaneously in cloud settings. 
Conclusion:
This experiment demonstrates how pruning and quantization can significantly enhance the  efficiency of generative AI models. While minor trade-offs in accuracy may occur, optimized  
models are much more suitable for real-time applications and resource-constrained  environments. These techniques are crucial for bringing the power of generative AI to  practical, everyday use cases. 
